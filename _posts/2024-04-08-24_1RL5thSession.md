---
title: "[06주차/강화학습 세션] DQN / DDQN"
excerpt: "DQN, 슬롯머신 구현" # 미리보기로 보이는 부분
categories: 24-1강화학습세션
tags: 
    - [강화학습, 정규세션]
toc: true
toc_sticky: true
comments: true
author: Jimin Lee

date: 2024-04-08

---

# 6주차 강화학습 세션

## 요약
- 이정연 벗께서 이미지 데이터 처리에 용이한 CNN 모델 발표를 해주셨습니다. 
- DQN, Double-Q learning, DDQN에 대해 학습했습니다. 
- 슬롯머신 문제를 환경부터 agent 코드까지 구현했습니다. 

- 구체적인 개인 발표 내용은 추후 **[개념정리] 카테고리**에서 확인하실 수 있습니다.  

## 개인 발표

- 📗 **CNN** : 이정연 벗

이미지 처리에 특화된 CNN 모델에 대해 설명해주셨습니다. CNN의 각 구성요소와 학습의 특징을 잘 짚어주셨습니다! 

## 강화학습 세션

- 📗 **DQN, Double-Q learning, DDQN, 강화학습 총정리**
- 📗 **다이나믹 프로그래밍의 환경 vs 강화학습의 환경**
- 👩‍💻 **슬롯머신 구현**

Q러닝의 가치 함수를 신경망으로 근사한 DQN에 대해 학습했습니다. DQN의 오프폴리쉬를 구현하는 방법, 리플레이 메모리의 특성과 DQN의 학습에 대해 설명했습니다. Q 러닝이 max Q값을 업데이트의 목표치로 설정하기 때문에 발생하는 overestimated 문제를 해결한 Double Q-learning을 소개하고, Double Q-learning과 DQN이 합쳐진 DDQN을 소개했습니다. 

다이나믹 프로그래밍부터 DQN까지 전반적인 강화학습의 발전역사를 훑었습니다. 각 방식이 갖고 있는 장단점을 쭉 리뷰하며 MDP 문제 해결을 위해 고안된 여러 방법론들의 계보를 훑을 수 있었습니다. 

02 세션 과제로 진행한 다이나믹 프로그래밍 환경 vs 강화학습 환경 비교를 발표했습니다. 주민서 벗이 대표로 환경 비교 분석을 발표해주셨습니다!! 다이나믹 프로그래밍은 모든 상태를 순차적으로 방문하며 값을 계산하고, 강화학습은 에피소드 단위로 상태를 방문하기 때문에 reset 코드가 꼭 필요하다는 점을 강조해주셨습니다. 

본격적인 구현에 들어가기 앞서, 간단한 강화학습 예제를 0부터 해결해나가는 과제를 수행했습니다. 3개의 레버가 달린 슬롯머신 환경을 구현하고, DQN Agent를 물리는 코드를 구현했습니다. 

## 사진
![IMG_9420](https://github.com/KanghwaSisters/kanghwasisters.github.io/assets/126959470/67f1308a-a695-4668-9410-c3677ffa1d5e)
![IMG_9422](https://github.com/KanghwaSisters/kanghwasisters.github.io/assets/126959470/c3900b08-6199-4935-8ff2-3133fc8b8d57)
![IMG_9669](https://github.com/KanghwaSisters/kanghwasisters.github.io/assets/126959470/dd5b9da6-e93a-4f71-855e-1eee471bde88)



